
name this folder as a package named hack_prompt_eval.
Include all folders, not just evals and utils.
I should import like `from hack_prompt_eval.utils.common_utils import attempt, log`



Run and check all evals.


Also add evaluating harmful/biased responses in all evals.


Evaluate latest LLMs.


Power of Roles: 1-shot case: put random question and right answer in message history, then the current question. This is a way for 1-shot prompting using the power of roles.


Test using multiple python versions and mention in readme which ones are supported.
Ask AI to suggest a better name.
	- hack_prompt_eval
Follow all standards for python packages.
Ask copilot to review.

Share on LinkedIn & X - both eval method and results.
	- Mention that this is old problem found years ago but not solved.
	- Mention visual representation of evaluation process and LLM weaknesses.

Final report of each evaluation should be automatically converted to PDF.
